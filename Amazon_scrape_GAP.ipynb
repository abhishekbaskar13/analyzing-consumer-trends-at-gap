{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing and Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BaVoUlyPurI6",
    "outputId": "515fdf82-6085-41a9-879d-aad98eeeb09c"
   },
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!apt-get update # to update ubuntu to correctly run apt install\n",
    "!apt install chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "import sys\n",
    "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
    "!pip install requests==2.31.0\n",
    "!pip install fake-useragent\n",
    "!pip install azure.ai.textanalytics\n",
    "!pip install azure.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "uGM7sq3atxmY"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import gzip\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "from urllib.request import Request, urlopen\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "import json\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function / Variable Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to amazon_product_reviews.csv\n",
      "Total reviews extracted: 92\n"
     ]
    }
   ],
   "source": [
    "# Function to request and retrieve HTML content of a given URL using Selenium\n",
    "# It sets up a headless Chrome browser, requests the page, waits for it to load, and scrolls to the bottom\n",
    "# The function returns the HTML of the page\n",
    "def request_page(url):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument(f'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "    driver = webdriver.Chrome(options=options)  \n",
    "    try:\n",
    "        driver.get(url) #Load the page\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        html = driver.page_source\n",
    "        return html\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Function to scrape Amazon reviews from multiple product URLs\n",
    "# It extracts product ID, name, price, review text, and date, and stores the results in a list\n",
    "def scrape_amazon_reviews(urls):\n",
    "    all_reviews_data = [] # List to hold all review data\n",
    "    \n",
    "    for url in urls:\n",
    "        html_content = request_page(url)\n",
    "        if html_content:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            # Extract Product ID and Name from URL\n",
    "            try:\n",
    "                product_id = url.split('/dp/')[1].split('/')[0]\n",
    "                product_name = url.split('/dp/')[0].split('/')[-1]  \n",
    "            except IndexError:\n",
    "                product_id = \"ID not found\"\n",
    "                product_name = \"Name not found\"\n",
    "            \n",
    "            price_element = soup.select_one('span.a-price-whole')\n",
    "            price = price_element.text if price_element else \"Price not found\"\n",
    "            review_elements = soup.select('div[data-hook=\"review\"]')\n",
    "            for review in review_elements:\n",
    "                content_element = review.select_one('span[data-hook=\"review-body\"] span')\n",
    "                review_text = content_element.text.strip() if content_element else \"Review not found\"\n",
    "                date_element = review.select_one('span[data-hook=\"review-date\"]')\n",
    "                review_date = date_element.text.strip() if date_element else \"Date not found\"\n",
    "                all_reviews_data.append({'Product ID': product_id, 'Product Name': product_name, 'Price': price, 'Review': review_text, 'Date': review_date, 'URL': url}) \n",
    "    return all_reviews_data\n",
    "\n",
    "# List of Amazon product URLs to scrape\n",
    "# Add more product URLs as needed...\n",
    "amazon_product_urls = [\n",
    "    'https://www.amazon.com/Amazon-Essentials-Classic-Fit-Wrinkle-Resistant-Flat-Front/dp/B01JQTGMIO/ref=cm_cr_arp_d_product_top?ie=UTF8&th=1&psc=1',\n",
    "    'https://www.amazon.com/GAP-Short-Sleeve-Outfit-GALAXY/dp/B0B6DFY3ST/ref=cm_cr_arp_d_product_top?ie=UTF8&th=1&psc=1',\n",
    "    'https://www.amazon.com/GAP-Boys-Skinny-Jeans-Black/dp/B0CCBC13YQ/ref=cm_cr_arp_d_product_top?ie=UTF8&th=1&psc=1',\n",
    "    'https://www.amazon.com/GAP-Untucked-Flannel-Button-Blackwatch/dp/B09VS6TS69/ref=cm_cr_arp_d_product_top?ie=UTF8&th=1&psc=1',\n",
    "    'https://www.amazon.com/GAP-Womens-Short-Sleeve-Uniform/dp/B0B6CX3RKT/ref=cm_cr_arp_d_product_top?ie=UTF8&th=1&psc=1',\n",
    "    'https://www.amazon.com/GAP-Girls-Sleeve-T-Shirt-Wednesday/dp/B09V8CC9NB/ref=cm_cr_arp_d_product_top?ie=UTF8&th=1&psc=1',\n",
    "    'https://www.amazon.com/GAP-Girls-Skater-Dress-Floral/dp/B0B6D4B8T4/ref=cm_cr_arp_d_product_top?ie=UTF8&th=1&psc=1',\n",
    "    'https://www.amazon.com/GAP-Girls-Pull-Joggers-MediumStandard/dp/B09W8Y9LX3/ref=cm_cr_arp_d_product_top?ie=UTF8&th=1&psc=1',\n",
    "    'https://www.amazon.com/GAP-282680-womens-Crew-Socks/dp/B09R3MX7LT/ref=cm_cr_arp_d_product_top?ie=UTF8&th=1&psc=1',\n",
    "    'https://www.amazon.com/GAP-Vintage-Sleeve-T-Shirt-X-Small/dp/B09VRXCPCW/ref=cm_cr_arp_d_product_top?ie=UTF8&th=1&psc=1'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Functions & Saving Solutions to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = scrape_amazon_reviews(amazon_product_urls)\n",
    "\n",
    "df = pd.DataFrame(all_reviews)\n",
    "df.to_csv('amazon_product_reviews.csv', index=False)\n",
    "\n",
    "print(\"Data has been saved to amazon_product_reviews.csv\")\n",
    "print(f\"Total reviews extracted: {len(all_reviews)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
